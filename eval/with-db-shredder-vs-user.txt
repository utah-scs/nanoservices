Time: 2020/09/07.

This is to compare shredder with vanilla user service (both single and multiple instances.)

Some number to give you guys a sense. 
This time, I intentially, make sure that, the database user-db is on a physical node that is different from the node running either user service or shredder.
for 1 core. vanilla user service can reach 800~900 req/s, while shredder can get 1500 req/s.

each of the following number just run once. need to run more for the paper.
each experiment run 60 seconds of duration.


core	user (1 container)	user(n contianer)  shredder
1 		851.53              851.53              1512.43  (shredder somehow only use <50% CPU here)
2       2069.74             1652.15             3450.21
4       5076.81             2715.84             7595.25  (here shredder only use <200% CPU)
8       7753.61             6912.04             19738.64 (here it uses 400%)
16      7596.90             19760.49            34073.57 (<800%)
20      7676.54             25177.80            33585.97 (<900%) --- mongodb is not saturated at this point, confirmed.


core	user (1 con)	    user(n con 2 node)  shredder(mongo)  shredder(non-mongo, 10, 100)
1 		851.53              851.53              1512.43         18115.79     23988.17    23390.43
2       2069.74             1652.15             3450.21         30292.73     37365.08    43366.58
4       5076.81             2715.84             7595.25         63692.50     76280.76    76756.77
8       7753.61             6912.04             19738.64        82012.98    151974.38   158042.84
16      7596.90             19760.49            34073.57        72493.07    201880.24   220137.44
20      7676.54             25177.80            33585.97        72009.64    216340.55   230587.66

NOTE1: shredder seems to have a bug on using just half the CPU capability.
NOTE2: I tried to increase the concurrency count to 1000 for shredder. but it crashes shredder. Too many files opened.
        licai@ctl:/mnt/extra/nano/deploy$ ERROR 2020-09-08 01:20:40,910 [shard 0] seastar - Exiting on unhandled exception: std::system_error (error system:24, accept4: Too many open files)
        shredder: /mnt/extra/nanoservices/seastar/include/seastar/core/sharded.hh:717: Service& seastar::sharded<T>::local() [with Service = scheduler]: Assertion `local_is_initialized()' failed.
NOTE3: forgot to mention. I used two compute nodes for user services. So this may be the reason when I use multiple containers the throughput is higher.
        I should have only use one compute node.

cmd:
for user:
<= 4 cores: (because otherwise it has lower throughput if I give it too many concurrent users)
    wrk -t 10 -c 10 -d 60 http://ctl:30002/login -H 'Authorization: Basic dXNlcjpwYXNzd29yZA=='
    e.g. when it is 4 core, 10 concurrency gets 5076.81 100 concurrency gets 3566.78
> 4 cores:
    wrk -t 10 -c 100 -d 60 http://ctl:30002/login -H 'Authorization: Basic dXNlcjpwYXNzd29yZA=='

for shredder:
wrk -t 10 -c 10 -d 10 http://ctl:11211/user/mongo_login -H 'Authorization: Basic dXNlcjpwYXNzd29yZA=='


user service cpu using increasing number of cores. memory using 2GB. if too small it will be OOM killed when the concurrency is too high.
mongodb has never saturated the CPU. for all the cases in user (1 container)
wrk has never saturated the CPU. for all the cases in user (1 container)


shredder without mongo. the clients are running on another physical node
